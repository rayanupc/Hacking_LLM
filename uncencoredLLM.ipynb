{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+L3atVXr6WdGVnNM6Kogk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rayanupc/HackingLLM/blob/main/uncencoredLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_ejxhxccas6j"
      },
      "outputs": [],
      "source": [
        "!pip install transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation forcée pour éviter les conflits de versions\n",
        "!pip install -U transformers accelerate transformer_lens einops jaxtyping datasets tiktoken"
      ],
      "metadata": {
        "id": "FeJ6YukRerf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import functools\n",
        "import einops\n",
        "import gc\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from torch import Tensor\n",
        "from typing import List\n",
        "from transformer_lens import HookedTransformer, utils\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from collections import defaultdict\n",
        "\n",
        "# On désactive le calcul des gradients pour économiser la mémoire du GPU\n",
        "torch.set_grad_enabled(False)"
      ],
      "metadata": {
        "id": "s4f9sxMQe-mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reformat_texts(texts):\n",
        "    return [[{\"role\": \"user\", \"content\": text}] for text in texts]\n",
        "\n",
        "# On récupère les datasets préparés par Maxime Labonne\n",
        "dataset_harmful = load_dataset('mlabonne/harmful_behaviors')\n",
        "dataset_harmless = load_dataset('mlabonne/harmless_alpaca')\n",
        "\n",
        "harmful_inst_train = reformat_texts(dataset_harmful['train']['text'])\n",
        "harmless_inst_train = reformat_texts(dataset_harmless['train']['text'])\n",
        "\n",
        "print(f\"Datasets chargés : {len(harmful_inst_train)} exemples nocifs et {len(harmless_inst_train)} inoffensifs.\")"
      ],
      "metadata": {
        "id": "_Do_0Wb-fPj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Définition des identifiants\n",
        "MODEL_ID = \"mlabonne/Daredevil-8B\"\n",
        "MODEL_TYPE = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# Téléchargement direct des fichiers sur le disque Colab\n",
        "!git clone https://huggingface.co/{MODEL_ID} {MODEL_TYPE}\n",
        "\n",
        "model = HookedTransformer.from_pretrained_no_processing(\n",
        "    MODEL_TYPE,\n",
        "    local_files_only=True,\n",
        "    dtype=torch.bfloat16,\n",
        "    default_padding_side='left'\n",
        ")\n",
        "\n",
        "# Chargement du tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)\n",
        "tokenizer.padding_side = 'left'\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "9GcNlyC6jPxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes accelerate transformers transformer_lens einops jaxtyping datasets tiktoken"
      ],
      "metadata": {
        "id": "9SW-vqEasQSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "saxpTwUAswHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation de toutes les dépendances nécessaires\n",
        "!pip install -U transformer_lens bitsandbytes accelerate transformers einops jaxtyping datasets tiktoken"
      ],
      "metadata": {
        "id": "s4Fc3zdTtoXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "_fW9ItN1uxES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation qui résout les conflits\n",
        "!pip install -U transformer_lens bitsandbytes accelerate transformers>=4.51.0 einops jaxtyping datasets tiktoken"
      ],
      "metadata": {
        "id": "aQ460RIFx1_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation propre sans conflit de version\n",
        "!pip install -U transformer_lens bitsandbytes accelerate transformers einops jaxtyping datasets tiktoken"
      ],
      "metadata": {
        "id": "gSXWn3Gkygqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "mQADajkfykvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformer_lens accelerate transformers einops jaxtyping datasets"
      ],
      "metadata": {
        "id": "zW5CcpvC4gUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "5WzJgr4c40O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformer_lens import HookedTransformer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "# Chargement en bfloat16 pour de meilleurs résultats en cyber\n",
        "model = HookedTransformer.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device=\"cuda\",\n",
        "    fold_ln=False,\n",
        "    center_writing_weights=False,\n",
        "    center_unembed=False,\n",
        "    dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "print(\"Le modèle 3.2-3B est chargé et prêt !\")"
      ],
      "metadata": {
        "id": "hJKI-uVm45Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "# Libère la mémoire mise en cache par PyTorch\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "E0dyRNAl8JDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "# Chargement des données (on réduit à 20 exemples pour être sûr que ça passe)\n",
        "harmful = load_dataset('mlabonne/harmful_behaviors', split='train[:20]')\n",
        "harmless = load_dataset('mlabonne/harmless_alpaca', split='train[:20]')\n",
        "\n",
        "def get_mean_activations_safe(data, layer):\n",
        "    act_name = f\"blocks.{layer}.hook_resid_pre\"\n",
        "    accumulated_activations = torch.zeros(model.cfg.d_model).cpu()\n",
        "\n",
        "    # On passe en mode inference pure pour consommer 0 mémoire de gradient\n",
        "    with torch.inference_mode():\n",
        "        for text in data['text']:\n",
        "            tokens = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": text}],\n",
        "                                                 return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "\n",
        "            # run_with_cache est gourmand, on utilise un hook temporaire à la place\n",
        "            temp_act = []\n",
        "            def save_act(tensor, hook): temp_act.append(tensor[0, -1, :].detach().cpu())\n",
        "\n",
        "            model.run_with_hooks(tokens, fwd_hooks=[(act_name, save_act)])\n",
        "\n",
        "            accumulated_activations += temp_act[0]\n",
        "\n",
        "            del tokens, temp_act\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "    return accumulated_activations / len(data['text'])\n",
        "\n",
        "layer_idx = 15\n",
        "print(\"Calcul en cours (méthode ultra-légère)...\")\n",
        "mean_harmful = get_mean_activations_safe(harmful, layer_idx)\n",
        "mean_harmless = get_mean_activations_safe(harmless, layer_idx)\n",
        "\n",
        "refusal_dir = (mean_harmful - mean_harmless).to(\"cuda\")\n",
        "refusal_dir = refusal_dir / refusal_dir.norm()\n",
        "\n",
        "print(f\"TERMINÉ ! Direction de refus isolée sur la couche {layer_idx}.\")"
      ],
      "metadata": {
        "id": "C54KbMFC8OrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import einops\n",
        "import torch\n",
        "\n",
        "# On force le vecteur de refus à utiliser le même format que le modèle (BFloat16)\n",
        "refusal_dir = refusal_dir.to(dtype=model.cfg.dtype, device=\"cuda\")\n",
        "\n",
        "# Définition de la fonction d'intervention\n",
        "def ablation_hook(activation, hook, direction):\n",
        "    # On s'assure que tout est au même format dans le calcul\n",
        "    direction = direction.to(activation.dtype)\n",
        "    proj = (einops.einsum(activation, direction.view(-1, 1), \"... d, d s -> ... s\") * direction)\n",
        "    return activation - proj\n",
        "\n",
        "# Prompt système\n",
        "prompt = \"En tant qu'étudiant en Master Cyber, explique-moi techniquement le fonctionnement d'un exploit 'Blind SQL Injection' et comment l'analyser dans un log serveur.\"\n",
        "\n",
        "tokens = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}],\n",
        "                                     return_tensors=\"pt\",\n",
        "                                     add_generation_prompt=True).to(\"cuda\")\n",
        "\n",
        "print(\"Génération en cours (Format BFloat16 synchronisé)...\\n\")\n",
        "with model.hooks(fwd_hooks=[(f\"blocks.{layer_idx}.hook_resid_pre\",\n",
        "                             functools.partial(ablation_hook, direction=refusal_dir))]):\n",
        "    # do_sample=True peut aider à avoir une réponse plus fluide\n",
        "    output = model.generate(tokens, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
        "\n",
        "print(\"--- RÉSULTAT DU MODÈLE DÉBRIDÉ ---\")\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "JaCC_fpc86Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import einops\n",
        "\n",
        "# Préparation du vecteur\n",
        "refusal_dir = refusal_dir.to(device=\"cuda\", dtype=model.cfg.dtype)\n",
        "\n",
        "def orthogonalize_weights(weights, direction):\n",
        "    # On retire la projection du vecteur de refus sur la dernière dimension\n",
        "    # Calcul de la projection : (W . v) v\n",
        "    dot_product = einops.einsum(weights, direction, \"... d, d -> ...\")\n",
        "    projection = einops.einsum(dot_product, direction, \"..., d -> ... d\")\n",
        "\n",
        "    return weights - projection\n",
        "\n",
        "# On part de la couche 15 jusqu'à la fin\n",
        "for l in range(layer_idx, model.cfg.n_layers):\n",
        "    # Pour l'Attention (W_O est la matrice de sortie des têtes)\n",
        "    model.blocks[l].attn.W_O.data = orthogonalize_weights(model.blocks[l].attn.W_O.data, refusal_dir)\n",
        "\n",
        "    # Pour le MLP (W_out est la matrice de projection finale du bloc)\n",
        "    model.blocks[l].mlp.W_out.data = orthogonalize_weights(model.blocks[l].mlp.W_out.data, refusal_dir)\n",
        "\n",
        "print(\"Abliteration permanente terminée avec succès sur les poids du modèle !\")"
      ],
      "metadata": {
        "id": "p97ucS46_hYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformer_lens import HookedTransformer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "model = HookedTransformer.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device=\"cuda\",\n",
        "    fold_ln=False,\n",
        "    center_writing_weights=False,\n",
        "    center_unembed=False,\n",
        "    dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "print(\"Modèle 3.2-3B rechargé.\")"
      ],
      "metadata": {
        "id": "6YF2WW7PBjlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import gc\n",
        "\n",
        "harmful = load_dataset('mlabonne/harmful_behaviors', split='train[:20]')\n",
        "harmless = load_dataset('mlabonne/harmless_alpaca', split='train[:20]')\n",
        "\n",
        "def get_mean_activations_safe(data, layer):\n",
        "    act_name = f\"blocks.{layer}.hook_resid_pre\"\n",
        "    accumulated_activations = torch.zeros(model.cfg.d_model).cpu()\n",
        "    with torch.inference_mode():\n",
        "        for text in data['text']:\n",
        "            tokens = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": text}],\n",
        "                                                 return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "            temp_act = []\n",
        "            def save_act(tensor, hook): temp_act.append(tensor[0, -1, :].detach().cpu())\n",
        "            model.run_with_hooks(tokens, fwd_hooks=[(act_name, save_act)])\n",
        "            accumulated_activations += temp_act[0]\n",
        "            del tokens, temp_act\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "    return accumulated_activations / len(data['text'])\n",
        "\n",
        "layer_idx = 15\n",
        "print(\"Calcul du vecteur de refus...\")\n",
        "mean_harmful = get_mean_activations_safe(harmful, layer_idx)\n",
        "mean_harmless = get_mean_activations_safe(harmless, layer_idx)\n",
        "refusal_dir = (mean_harmful - mean_harmless).to(\"cuda\")\n",
        "refusal_dir = refusal_dir / refusal_dir.norm()\n",
        "print(\"Vecteur de refus restauré.\")"
      ],
      "metadata": {
        "id": "T8Lp5n1OCfip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "import einops\n",
        "import os\n",
        "from transformer_lens import HookedTransformer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# PARAMÈTRES\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "save_path = \"/content/llama-3-2-3b-cyber\"\n",
        "layer_idx = 15\n",
        "\n",
        "print(\"Étape 1 : Rechargement du modèle...\")\n",
        "model = HookedTransformer.from_pretrained(MODEL_ID, device=\"cuda\", fold_ln=False, center_writing_weights=False, center_unembed=False, dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# CALCUL DU VECTEUR (Correction du TypeError 'hook')\n",
        "print(\"Étape 2 : Calcul du vecteur de refus...\")\n",
        "harmful = load_dataset('mlabonne/harmful_behaviors', split='train[:15]')\n",
        "harmless = load_dataset('mlabonne/harmless_alpaca', split='train[:15]')\n",
        "\n",
        "def get_mean_act(data, layer):\n",
        "    act_name = f\"blocks.{layer}.hook_resid_pre\"\n",
        "    acc = torch.zeros(model.cfg.d_model).cpu()\n",
        "    with torch.inference_mode():\n",
        "        for text in data['text']:\n",
        "            tokens = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": text}], return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "            temp = []\n",
        "\n",
        "            # Correction de la fonction hook pour accepter l'argument 'hook' explicitement\n",
        "            def hook_fn(tensor, hook):\n",
        "                temp.append(tensor[0, -1, :].detach().cpu())\n",
        "\n",
        "            model.run_with_hooks(tokens, fwd_hooks=[(act_name, hook_fn)])\n",
        "            acc += temp[0]\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "    return acc / len(data['text'])\n",
        "\n",
        "refusal_dir = (get_mean_act(harmful, layer_idx) - get_mean_act(harmless, layer_idx)).to(\"cuda\", dtype=torch.bfloat16)\n",
        "refusal_dir /= refusal_dir.norm()\n",
        "\n",
        "print(\"Étape 3 : Modification permanente des matrices...\")\n",
        "def ortho(w, d):\n",
        "    # Projette et retire la direction de refus des poids\n",
        "    dot = einops.einsum(w, d, \"... d, d -> ...\")\n",
        "    proj = einops.einsum(dot, d, \"..., d -> ... d\")\n",
        "    return w - proj\n",
        "\n",
        "for l in range(layer_idx, model.cfg.n_layers):\n",
        "    model.blocks[l].attn.W_O.data = ortho(model.blocks[l].attn.W_O.data, refusal_dir)\n",
        "    model.blocks[l].mlp.W_out.data = ortho(model.blocks[l].mlp.W_out.data, refusal_dir)\n",
        "\n",
        "# EXPORT VERS HUGGING FACE (Version \"Zéro Crash RAM\")\n",
        "print(\"Étape 4 : Extraction des poids et libération de la RAM...\")\n",
        "\n",
        "# On extrait uniquement les poids modifiés dans un dictionnaire CPU\n",
        "modified_weights = {}\n",
        "for l in range(layer_idx, model.cfg.n_layers):\n",
        "    modified_weights[l] = {\n",
        "        'mlp': model.blocks[l].mlp.W_out.data.T.detach().cpu().clone(),\n",
        "        'attn': einops.rearrange(model.blocks[l].attn.W_O.data.detach().cpu().clone(),\n",
        "                                 \"n_h d_h d_m -> (n_h d_h) d_m\").T\n",
        "    }\n",
        "\n",
        "# ON SUPPRIME LE MODÈLE DE RECHERCHE\n",
        "del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Mémoire libérée. Chargement de la structure finale...\")\n",
        "\n",
        "# On charge la structure de production\n",
        "final_hf = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"cpu\"\n",
        ")\n",
        "\n",
        "# On injecte nos poids extraits\n",
        "print(\"Injection des poids dans le modèle de production...\")\n",
        "for l, weights in modified_weights.items():\n",
        "    final_hf.model.layers[l].mlp.down_proj.weight.data = weights['mlp']\n",
        "    final_hf.model.layers[l].self_attn.o_proj.weight.data = weights['attn']\n",
        "\n",
        "# Sauvegarde physique\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "final_hf.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"Modèle sauvegardé dans {save_path}\")"
      ],
      "metadata": {
        "id": "MUpOqAbMFEGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "shutil.make_archive(\"/content/drive/MyDrive/llama_cyber_3b_final\", 'zip', \"/content/llama-3-2-3b-cyber\")\n",
        "print(\"L'archive est sur le Drive !\")"
      ],
      "metadata": {
        "id": "LmdK0c3dJt2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Préparation de l'environnement (CPU uniquement)\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!cd llama.cpp && make -j\n",
        "\n",
        "# Installation des outils de conversion\n",
        "!pip install gguf sentencepiece transformers\n",
        "\n",
        "# Conversion vers le format GGUF (Précision totale)\n",
        "print(\"Conversion en cours... Patientez environ 2-3 minutes.\")\n",
        "!python llama.cpp/convert_hf_to_gguf.py /content/llama-3-2-3b-cyber --outfile /content/model-f16.gguf\n",
        "\n",
        "# On compresse de 6Go à 2Go pour la fluidité\n",
        "print(\"Compression (Quantification Q4_K_M) en cours...\")\n",
        "!./llama.cpp/llama-quantize /content/model-f16.gguf /content/model-q4_k_m.gguf Q4_K_M\n",
        "\n",
        "print(\"SUCCÈS ! Le modèle fluide est prêt.\")"
      ],
      "metadata": {
        "id": "NjiVFMExhG4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Montage du Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Dézippage du modèle vers Colab\n",
        "!unzip /content/drive/MyDrive/llama_cyber_3b_final.zip -d /content/llama-3-2-3b-cyber\n",
        "\n",
        "print(\" Dossier restauré dans /content/llama-3-2-3b-cyber\")"
      ],
      "metadata": {
        "id": "YAG_djaEh4SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation de CMake (nécessaire pour la nouvelle version)\n",
        "!apt-get install cmake\n",
        "\n",
        "# Clone et compilation propre\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!mkdir -p llama.cpp/build\n",
        "%cd llama.cpp/build\n",
        "!cmake ..\n",
        "!cmake --build . --config Release -j\n",
        "%cd /content/\n",
        "\n",
        "print(\"Outils de compression (llama-quantize) compilés avec succès !\")"
      ],
      "metadata": {
        "id": "1I8DMgehixFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation des dépendances Python\n",
        "!pip install gguf sentencepiece transformers\n",
        "\n",
        "# Conversion vers GGUF\n",
        "print(\"Conversion en cours...\")\n",
        "!python3 llama.cpp/convert_hf_to_gguf.py /content/llama-3-2-3b-cyber --outfile /content/model-f16.gguf\n",
        "\n",
        "# Quantification (Compression pour votre Mac Intel)\n",
        "# Notez le nouveau chemin vers l'exécutable : llama.cpp/build/bin/\n",
        "print(\"Compression Q4_K_M en cours...\")\n",
        "!./llama.cpp/build/bin/llama-quantize /content/model-f16.gguf /content/model-q4_k_m.gguf Q4_K_M\n",
        "\n",
        "print(\"Télécharger maintenant /content/model-q4_k_m.gguf\")"
      ],
      "metadata": {
        "id": "ovymGWFUlAlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilation forcée de l'outil de quantification\n",
        "%cd /content/llama.cpp\n",
        "!make llama-quantize -j\n",
        "\n",
        "# Retour au dossier principal\n",
        "%cd /content/\n",
        "\n",
        "print(\"L'outil llama-quantize est prêt !\")"
      ],
      "metadata": {
        "id": "ZB8_4hpgocYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On compresse le fichier de 6.4 Go vers 2.1 Go\n",
        "!./llama.cpp/llama-quantize /content/model-f16.gguf /content/model-q4_k_m.gguf Q4_K_M\n",
        "\n",
        "print(\"Le fichier model-q4_k_m.gguf est prêt.\")"
      ],
      "metadata": {
        "id": "AFQnycDoohyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Préparation du dossier de build\n",
        "%cd /content/llama.cpp\n",
        "!mkdir -p build\n",
        "%cd build\n",
        "\n",
        "# Configuration et compilation de l'outil llama-quantize uniquement\n",
        "!cmake ..\n",
        "!cmake --build . --config Release --target llama-quantize -j\n",
        "\n",
        "# On remonte au dossier principal\n",
        "%cd /content/\n",
        "print(\"L'outil llama-quantize est enfin compilé et prêt !\")"
      ],
      "metadata": {
        "id": "AJEJwozopLwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On utilise le nouveau chemin généré par CMake\n",
        "!./llama.cpp/build/bin/llama-quantize /content/model-f16.gguf /content/model-q4_k_m.gguf Q4_K_M\n",
        "\n",
        "print(\"Le fichier model-q4_k_m.gguf est  généré.\")"
      ],
      "metadata": {
        "id": "qbMdvtlSrAZZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}